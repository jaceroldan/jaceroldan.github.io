---
layout: post
title:  "CuatroLLM niche translations with adaptive in-context learning: exploring English-Filipino"
---

# Summary

This mini-project focused on CuatroLLM, a 1.3B parameter model pre-trained on four languages: English, French, Spanish, and German. We replicated its baseline results on complex reasoning tasks. Fine-tuning CuatroLLM with a scientific corpus marginally worsens performance in complex reasoning. However, in zero-shot and one-shot scenarios, we demonstrate improvements in fine-tuned CuatroLLMâ€™s English-French translation for the scientific domain. Lastly, we explored CuatroLLM for English-Filipino translation with a small compiled dataset. Including Filipino as part of pre-training could improve this performance, but it would require a larger translation dataset.

# Methods

The baseline model is the pre-trained CuatroLLM, generated by training from scratch a \emph{Mistral-7B-Instruct-v0.1} model on a quadrilingual dataset, \emph{TransWeb-Edu}, which the authors compiled. We then compared the baseline performance with two fine-tuned versions: (1) CuatroLLM generated by the authors by continuous training with additional RedPajama-v2 dataset and cooldown datasets that included Python-Edu, WebInstruct, and WebInstruct-French; and (2) CuatroLLM fine-tuned via supervised fine-tuning training (SFTT) through the in-context learning with human feedback developed by Moslem et al. for adaptive MT.

The SFTT employed a parameter-efficient fine-tuning (PEFT)~\cite{peft} with a low-rank adaptive (LoRA) alpha of 16 and dropout of 0.1. This PEFT model generated a version of the pre-trained CuatroLLM that is lighter to fine-tune with a few GPUs. We froze the tokenizer during SFFT training, which is contrary to the fine-tuning method applied by Wang et al. to obtain their fine-tuned \emph{CuatroLLM-cool}~. We then evaluated the complex reasoning task performance of the baseline and fine-tuned CuatroLLM. We replicated these metrics for baseline CuatroLLM on the four languages that CuatroLLM had pre-training.

Fine-tuning datasets from Moslem et al. were available for French and Spanish. We fine-tuned CuatroLLM on these languages using the adaptive MT approach and compared their CuatroBen metrics to the baseline model. Subsequently, we assessed the CuatroLLM MT performance for English-French translations using BLEU, chrF++, and TER. We examined both zero- and one-shot translations and compared them to their corresponding results reported in the paper.

Finally, we fine-tuned a baseline CuatroLLM for English-Filipino translations even if CuatroLLM did not have pre-training in Filipino. We compared CuatroLLM's performance for this type of translation with NLLB, which is a suitable benchmark.

# Methodology and Rationale of Proposed Solution



| Run | BoxP  | R     | mAP   | Remarks                                                                                                                             |
|-----|-------|-------|-------|-------------------------------------------------------------------------------------------------------------------------------------|
| 1   | 0.910 | 0.800 | 0.879 | Default YOLO-V8 nano, 10 epochs                                                                                                     |
| 2   | 0.946 | 0.901 | 0.922 | Run 1 + Default YOLO-V11 nano                                                                                                       |
| 3   | 0.939 | 0.885 | 0.923 | Run 1 + mosaic=0.5, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.373, translate=0.45, scale=0.5, shear=0.3, flipud=0.01, fliplr=0.5 |                                                                      |
| 4   | 0.947 | 0.921 | 0.941 | Run3 with YOLO-V8 medium                                                                                                            |
| 5   | 0.955 | 0.912 | 0.940 | Run3 with YOLO-V11 medium                                                                                                           |
| 6   | 0.951 | 0.919 | 0.940 | Run3 with YOLO-V11 X                                                                                                                |
| 7   | 0.951 | 0.918 | 0.938 | Run6 with CosLR 0.01 to 0.001                                                                                                       |
| 8   | 0.964 | 0.933 | 0.948 | Run6 with 20 epochs                                                                                                                 |
| 9   | 0.964 | 0.958 | 0.974 | Run6 with 30 epochs                                                                                                                 |
| 10   | 0.968 | 0.969 | 0.977 | Run9 with Mixup variation on 30 epochs                                                                                             |
| 11   | 0.969 | 0.977 | 0.974 | Run9 with Mixup variation on 50 epochs                                                                                             |
| 12   | 0.883 | 0.773 | 0.844 | Made a dataset improvement and reran using Trial 11 settings on 30 epochs                                                          |
| 13   | 0.870 | 0.763 | 0.828 | Trial 12 but on 50 epochs                                                                                                          |
| 14   | 0.898 | 0.785 | 0.842 | Made further dataset improvments and reran using Trial 11 on 30 epochs                                                             |
| 15   | 0.850 | 0.775 | 0.830 | Trial 14 with HSV_h 0.25                                                                                                           |
| 16   | 0.879 | 0.757 | 0.820 | Trial 14 with HSV_h 0.35                                                                                                           |
| 17   | 0.845 | 0.775 | 0.817 | Trial 14 with HSV_h 0.45                                                                                                           |
| 18   | 0.877 | 0.766 | 0.832 | Trial 14 with HSV_s 0.6                                                                                                            |
| 19   | 0.874 | 0.798 | 0.845 | Trial 14 with HSV_s 0.5                                                                                                            |
| 20   | 0.907 | 0.777 | 0.842 | Trial 14 with HSV_s 0.4                                                                                                            |
| 21   | 0.865 | 0.779 | 0.833 | Trial 14 with HSV_v 0.5                                                                                                            |
| 22   | 0.846 | 0.785 | 0.839 | Trial 14 with HSV_v 0.6                                                                                                            |
| 23   | 0.859 | 0.785 | 0.974 | Trial 14 with HSV_v 0.7                                                                                                            |
